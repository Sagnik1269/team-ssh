{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_from_disk\n",
    "# import streamlit as st\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_from_disk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "nltk.data.path.append(os.path.expanduser('~/nltk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate an embedding for a query using a pre-trained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, code_embeddings, model, tokenizer, dataset, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using cosine similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_query(query, model, tokenizer)\n",
    "    similarities = cosine_similarity(query_embedding, code_embeddings)\n",
    "\n",
    "    top_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
    "    return top_indices, similarities[0][top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results for query: 'Transpose the columns into rows, remove all of the rows that are empty after the first cell, then     transpose back. '\n",
      "\n",
      "Index: 990, Similarity Score: 0.9802\n",
      "Code Snippet:\n",
      "def drop_empty(rows):\n",
      "    \"\"\"Transpose the columns into rows, remove all of the rows that are empty after the first cell, then\n",
      "    transpose back. The result is that columns that have a header but no data in the body are removed, assuming\n",
      "    the header is the first row. \"\"\"\n",
      "    return zip(*[col for col in zip(*rows) if bool(filter(bool, col[1:]))])\n",
      "\n",
      "Description:\n",
      "Transpose the columns into rows, remove all of the rows that are empty after the first cell, then\n",
      "    transpose back. The result is that columns that have a header but no data in the body are removed, assuming\n",
      "    the header is the first row.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 779, Similarity Score: 0.9687\n",
      "Code Snippet:\n",
      "def get_naive(dt):\n",
      "  \"\"\"Gets a naive datetime from a datetime.\n",
      "\n",
      "  datetime_tz objects can't just have tzinfo replaced with None, you need to\n",
      "  call asdatetime.\n",
      "\n",
      "  Args:\n",
      "    dt: datetime object.\n",
      "\n",
      "  Returns:\n",
      "    datetime object without any timezone information.\n",
      "  \"\"\"\n",
      "  if not dt.tzinfo:\n",
      "    return dt\n",
      "  if hasattr(dt, \"asdatetime\"):\n",
      "    return dt.asdatetime()\n",
      "  return dt.replace(tzinfo=None)\n",
      "\n",
      "Description:\n",
      "Gets a naive datetime from a datetime.\n",
      "\n",
      "  datetime_tz objects can't just have tzinfo replaced with None, you need to\n",
      "  call asdatetime.\n",
      "\n",
      "  Args:\n",
      "    dt: datetime object.\n",
      "\n",
      "  Returns:\n",
      "    datetime object without any timezone information.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 989, Similarity Score: 0.9657\n",
      "Code Snippet:\n",
      "def qualified_name_import(cls):\n",
      "    \"\"\"Full name of a class, including the module. Like qualified_class_name, but when you already have a class \"\"\"\n",
      "\n",
      "    parts = qualified_name(cls).split('.')\n",
      "\n",
      "    return \"from {} import {}\".format('.'.join(parts[:-1]), parts[-1])\n",
      "\n",
      "Description:\n",
      "Full name of a class, including the module. Like qualified_class_name, but when you already have a class\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 859, Similarity Score: 0.9649\n",
      "Code Snippet:\n",
      "def bash_echo_cooler(n):\n",
      "    \"\"\"A very basic example of how to destroy n running tasks\n",
      "    This is a cooler function\n",
      "    \"\"\"\n",
      "    import subprocess\n",
      "    cmd = (\n",
      "        'set -o pipefile '\n",
      "        ' ; kill `pgrep -f \"from bash: started relay launcher task\"'\n",
      "        ' | tail -n %s` 2>/dev/null' % n)\n",
      "    subprocess.Popen(cmd, shell=True, executable='bash').wait()\n",
      "\n",
      "Description:\n",
      "A very basic example of how to destroy n running tasks\n",
      "    This is a cooler function\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 538, Similarity Score: 0.9646\n",
      "Code Snippet:\n",
      "def replicate_filter(sources, model, cache=None):\n",
      "    '''Replicates the list of objects to other class and returns their\n",
      "    reflections'''\n",
      "    targets = [replicate_no_merge(source, model, cache=cache)\n",
      "               for source in sources]\n",
      "    # Some objects may not be available in target DB (not published), so we\n",
      "    # have to exclude None from the list.\n",
      "    return [target for target in targets if target is not None]\n",
      "\n",
      "Description:\n",
      "Replicates the list of objects to other class and returns their\n",
      "    reflections\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Load the code embeddings\n",
    "code_embeddings = np.load('code_embeddings.npy')\n",
    "\n",
    "dataset = load_from_disk('processed_dataset')\n",
    "while True:\n",
    "    query = input(\"Enter your search query (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "        # Perform semantic search\n",
    "    top_indices, scores = semantic_search(query, code_embeddings, model, tokenizer, dataset)\n",
    "    print(f\"\\nTop {len(top_indices)} results for query: '{query}'\\n\")\n",
    "    for idx, score in zip(top_indices, scores):\n",
    "        idx = int(idx)  # Convert numpy.int64 to Python int\n",
    "        print(f\"Index: {idx}, Similarity Score: {score:.4f}\")\n",
    "        print(f\"Code Snippet:\\n{dataset[idx]['code']}\\n\")\n",
    "        print(f\"Description:\\n{dataset[idx]['description']}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json \n",
    "import os\n",
    "def read_zip_file(zip_file_path):\n",
    "    zip_content = {}\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            with zip_ref.open(file_info) as file:\n",
    "                if file_info.filename.endswith('.jsonl.gz'):\n",
    "                    # Handle gzipped JSONL files\n",
    "                    content = gzip.decompress(file.read()).decode('utf-8')\n",
    "                    json_content = [json.loads(line) for line in content.splitlines()]\n",
    "                    zip_content[file_info.filename] = json_content\n",
    "                elif file_info.filename.endswith('.jsonl'):\n",
    "                    # Handle plain JSONL files\n",
    "                    content = file.read().decode('utf-8')\n",
    "                    json_content = [json.loads(line) for line in content.splitlines()]\n",
    "                    zip_content[file_info.filename] = json_content\n",
    "                elif file_info.filename.endswith('/'):\n",
    "                    # Skip directory entries\n",
    "                    continue\n",
    "                else:\n",
    "                    # For other file types, store as plain text\n",
    "                    content = file.read().decode('utf-8', errors='ignore')\n",
    "                    zip_content[file_info.filename] = content\n",
    "    return zip_content\n",
    "\n",
    "def read_all_files(folder_path):\n",
    "    all_content = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        print(filename)\n",
    "        if filename.endswith('.zip'):\n",
    "            zip_content = read_zip_file(os.path.join(folder_path, filename))\n",
    "        all_content.update(zip_content)\n",
    "        break\n",
    "    # print(all_content)\n",
    "    return all_content\n",
    "\n",
    "read_all_files('/home/admin/huggingFace_dataset/datasets/python_data/python/final/jsonl/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resources():\n",
    "    # nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    model_name = \"microsoft/codebert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Load code embeddings and dataset\n",
    "    code_embeddings = np.load('code_embeddings.npy')\n",
    "    dataset = load_from_disk('processed_dataset')\n",
    "\n",
    "    # Prepare BM25\n",
    "    \n",
    "    # tokenized_corpus = [word_tokenize(doc['code'].lower()) for doc in dataset]\n",
    "    tokenized_corpus = [doc['code'].lower().split() for doc in dataset]\n",
    "    \n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    return tokenizer, model, code_embeddings, dataset, bm25\n",
    "\n",
    "def embed_query(query, model, tokenizer):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def semantic_search(query, code_embeddings, model, tokenizer, dataset, top_k=5):\n",
    "    query_embedding = embed_query(query, model, tokenizer)\n",
    "    similarities = cosine_similarity(query_embedding, code_embeddings)\n",
    "    top_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
    "    return top_indices, similarities[0][top_indices]\n",
    "\n",
    "def bm25_search(query, bm25, top_k=5):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return top_indices, scores[top_indices]\n",
    "\n",
    "\n",
    "def hybrid_search(query, bm25, code_embeddings, model, tokenizer, dataset, top_k=5, alpha=0.5):\n",
    "    # BM25\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Semantic\n",
    "    query_embedding = embed_query(query, model, tokenizer)\n",
    "    semantic_scores = cosine_similarity(query_embedding, code_embeddings).flatten()\n",
    "    \n",
    "    # Normalize scores\n",
    "    bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
    "    semantic_norm = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min() + 1e-10)\n",
    "    \n",
    "    # Combine\n",
    "    combined_scores = alpha * bm25_norm + (1 - alpha) * semantic_norm\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    return top_indices, combined_scores[top_indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 19:13:06.750 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:06.842 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/admin/.local/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-10-20 19:13:06.843 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.021 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.021 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.022 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.022 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.022 Session state does not function when running a script without `streamlit run`\n",
      "2024-10-20 19:13:08.022 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 19:13:08.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    st.title(\"üîç Semantic and BM25 Code Search\")\n",
    "\n",
    "    # Load resources\n",
    "    tokenizer, model, code_embeddings, dataset, bm25 = load_resources()\n",
    "\n",
    "    # User input\n",
    "    query = st.text_input(\"Enter your search query:\", \"\")\n",
    "\n",
    "    if st.button(\"Search\"):\n",
    "        if query:\n",
    "            st.markdown(\"### üîπ Semantic Search Results:\")\n",
    "            top_indices_sem, scores_sem = semantic_search(query, code_embeddings, model, tokenizer, dataset)\n",
    "            for idx, score in zip(top_indices_sem, scores_sem):\n",
    "                idx = int(idx)\n",
    "                st.write(f\"**Index:** {idx} | **Similarity Score:** {score:.4f}\")\n",
    "                st.code(dataset[idx]['code'], language='python')  # Adjust language as needed\n",
    "                st.write(f\"**Description:** {dataset[idx]['description']}\")\n",
    "                st.markdown(\"---\")\n",
    "\n",
    "            st.markdown(\"### üî∏ BM25 Search Results:\")\n",
    "            top_indices_bm25, scores_bm25 = bm25_search(query, bm25)\n",
    "            for idx, score in zip(top_indices_bm25, scores_bm25):\n",
    "                idx = int(idx)\n",
    "                st.write(f\"**Index:** {idx} | **BM25 Score:** {score:.4f}\")\n",
    "                st.code(dataset[idx]['code'], language='python')  # Adjust language as needed\n",
    "                st.write(f\"**Description:** {dataset[idx]['description']}\")\n",
    "                st.markdown(\"---\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a search query.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 990, Similarity Score: 1.0000\n",
      "Code Snippet:\n",
      "def drop_empty(rows):\n",
      "    \"\"\"Transpose the columns into rows, remove all of the rows that are empty after the first cell, then\n",
      "    transpose back. The result is that columns that have a header but no data in the body are removed, assuming\n",
      "    the header is the first row. \"\"\"\n",
      "    return zip(*[col for col in zip(*rows) if bool(filter(bool, col[1:]))])\n",
      "\n",
      "Description:\n",
      "Transpose the columns into rows, remove all of the rows that are empty after the first cell, then\n",
      "    transpose back. The result is that columns that have a header but no data in the body are removed, assuming\n",
      "    the header is the first row.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 290, Similarity Score: 0.5500\n",
      "Code Snippet:\n",
      "def _index_document(self, document, force=False):\n",
      "        \"\"\" Adds dataset document to the index. \"\"\"\n",
      "        query = text(\"\"\"\n",
      "            INSERT INTO dataset_index(vid, title, keywords, doc)\n",
      "            VALUES(:vid, :title, string_to_array(:keywords, ' '), to_tsvector('english', :doc));\n",
      "        \"\"\")\n",
      "        self.execute(query, **document)\n",
      "\n",
      "Description:\n",
      "Adds dataset document to the index.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 308, Similarity Score: 0.5499\n",
      "Code Snippet:\n",
      "def primary_dimensions(self):\n",
      "        \"\"\"Iterate over the primary dimension columns, columns which do not have a parent\n",
      "\n",
      "        \"\"\"\n",
      "        from ambry.valuetype.core import ROLE\n",
      "\n",
      "        for c in self.columns:\n",
      "\n",
      "            if not c.parent and c.role == ROLE.DIMENSION:\n",
      "                    yield c\n",
      "\n",
      "Description:\n",
      "Iterate over the primary dimension columns, columns which do not have a parent\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 533, Similarity Score: 0.5493\n",
      "Code Snippet:\n",
      "def exclude(prop):\n",
      "    '''Don't replicate property that is normally replicated: ordering column,\n",
      "    many-to-one relation that is marked for replication from other side.'''\n",
      "    if isinstance(prop, QueryableAttribute):\n",
      "        prop = prop.property\n",
      "    assert isinstance(prop, (Column, ColumnProperty, RelationshipProperty))\n",
      "    _excluded.add(prop)\n",
      "    if isinstance(prop, RelationshipProperty):\n",
      "        # Also exclude columns that participate in this relationship\n",
      "        for local in prop.local_columns:\n",
      "            _excluded.add(local)\n",
      "\n",
      "Description:\n",
      "Don't replicate property that is normally replicated: ordering column,\n",
      "    many-to-one relation that is marked for replication from other side.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 309, Similarity Score: 0.5444\n",
      "Code Snippet:\n",
      "def primary_measures(self):\n",
      "        \"\"\"Iterate over the primary columns, columns which do not have a parent\n",
      "\n",
      "        Also sets the property partition_stats to the stats collection for the partition and column.\n",
      "        \"\"\"\n",
      "        from ambry.valuetype.core import ROLE\n",
      "\n",
      "        for c in self.columns:\n",
      "\n",
      "            if not c.parent and c.role == ROLE.MEASURE:\n",
      "                    yield c\n",
      "\n",
      "Description:\n",
      "Iterate over the primary columns, columns which do not have a parent\n",
      "\n",
      "        Also sets the property partition_stats to the stats collection for the partition and column.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, code_embeddings, dataset, bm25 = load_resources()\n",
    "query = input(\"Enter your search query (or type 'exit' to quit): \")\n",
    "\n",
    "top_indices, scores = hybrid_search(query, bm25, code_embeddings, model, tokenizer, dataset, top_k=5, alpha=0.5)\n",
    "\n",
    "for idx, score in zip(top_indices, scores):\n",
    "    idx = int(idx)  # Convert numpy.int64 to Python int\n",
    "    print(f\"Index: {idx}, Similarity Score: {score:.4f}\")\n",
    "    print(f\"Code Snippet:\\n{dataset[idx]['code']}\\n\")\n",
    "    print(f\"Description:\\n{dataset[idx]['description']}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/admin/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
